{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b04a68-ebac-4076-be0f-990f71f57593",
   "metadata": {},
   "source": [
    "# <center> Bank Telemarketing deposit \n",
    "# <center>Optimizing Long-Term Deposit Subscription Campaigns: Analyzing and Predicting potential clients </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703dbf9f-dc2e-4c9e-ae50-c8043bb535b5",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f6f2a-868b-4752-ba0b-154e489d82c3",
   "metadata": {},
   "source": [
    "#### 1. Importation of the necessary libraries to run the code\n",
    "#### 2. Loading the dataset\n",
    "#### 3. Exploratory Data Analysis (EDA)\n",
    "#### 4. Imputation of the missing values\n",
    "#### 5. Encoding categorical variables using pd.get_dummies\n",
    "#### 6. Application of SMOTE method to rebalance the classes\n",
    "#### 7. Selection of important features using Random Forest\n",
    "#### 8. Sélection des variables importantes avec ACP\n",
    "#### 9. Model development and evaluation using ROC AUC and confusion matrix\n",
    "#### 10. Comparison of results between ACP & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abed63-fbcb-4773-99d7-b288af9d2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importation of the necessary libraries to run the code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration of visual styles\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238154c8-9ce1-4502-b046-91ebde49c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Loading the dataset\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads the CSV file containing the data.\n",
    "    :param filepath: Path to the CSV file.\n",
    "    :return: DataFrame Pandas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=';')\n",
    "        print(\"Dataset loaded successfully\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading the dataset : {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to the data file\n",
    "filepath = \"/Users/abdel_merhom/Desktop/bank_marketing.csv\"\n",
    "\n",
    "# Loading the dataset\n",
    "data = load_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa6b4c-04ee-40ee-90fb-f9be5db47c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "def exploratory_data_analysis(df, target_col='y'):\n",
    "    \"\"\"\n",
    "    EDA is the first important step to understand our data. It will allows us to gain insights into the structure, content, different\n",
    "    patterns and relationships within the dataset.\n",
    "    \n",
    "    :param df: DataFrame.\n",
    "    :param target_col: Name of the target value (Response value).\n",
    "    \"\"\"\n",
    "    print(\"\\n---Exploratory Data Analysis---\")\n",
    "\n",
    "    # General informations\n",
    "    print(\"\\nInformations about the DataFrame :\")\n",
    "    display(df.info())\n",
    "\n",
    "    # Descriptive statistics\n",
    "    print(\"\\nDescriptive statistics for Numerical variables :\")\n",
    "    display(df.describe())\n",
    "\n",
    "    # Checking for missing values\n",
    "    print(\"\\nMissing values per column :\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    display(missing_values)\n",
    "\n",
    "    # Count of the occurences of \"unknown\"\n",
    "    print(\"\\nCounting occurences of 'unknown' :\")\n",
    "    unknown_counts = (df == 'unknown').sum()\n",
    "    display(unknown_counts)\n",
    "\n",
    "    # Distribution of the target variable\n",
    "    print(\"\\nDistribution of the target variable 'y' :\")\n",
    "    target_distribution = df[target_col].value_counts(normalize=True) * 100\n",
    "    display(target_distribution)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(data=df, x=target_col)\n",
    "    plt.title('Distribution of the target variable (y)')\n",
    "    plt.show()\n",
    "\n",
    "    # Analysis of categorical variables in relation to the target\n",
    "    categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', \"poutcome\"]\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nRépartition de '{col}' par rapport à '{target_col}':\")\n",
    "        pivot_table = pd.crosstab(df[col], df[target_col], normalize='index').round(2) * 100\n",
    "        display(pivot_table)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(data=df, x=col, hue=target_col, palette='Set3')\n",
    "        plt.title(f\"Répartition de '{col}' par rapport à '{target_col}'\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Analysis of numerical variables in relation to the target\n",
    "    numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'euribor3m']\n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\nMoyenne de '{col}' pour chaque classe de '{target_col}':\")\n",
    "        display(df.groupby(target_col)[col].mean())\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxplot(data=df, x=target_col, y=col, palette='Set2')\n",
    "        plt.title(f\"Relation entre '{col}' et '{target_col}'\")\n",
    "        plt.xlabel(target_col.capitalize())\n",
    "        plt.ylabel(col.capitalize())\n",
    "        plt.show()\n",
    "\n",
    "# Exécution of the EDA\n",
    "exploratory_data_analysis(data, target_col='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddd806-cd78-4248-8a5c-caa9c222bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Imputation of the missing values\n",
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values (\"unknown\") in categorical variables with the mode,and in numerical variables with the mean.\n",
    "    :param df: DataFrame.\n",
    "    :return: DataFrame after imputation.\n",
    "    \"\"\"\n",
    "    # Categorical variables\n",
    "    categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "    for col in categorical_cols:\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col] = df[col].replace('unknown', mode_value)\n",
    "\n",
    "    # Numerical variables\n",
    "    numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "    for col in numeric_cols:\n",
    "        mean_value = df[col].mean()\n",
    "        df[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "    print(\"Missing values imputed successfully..\")\n",
    "    return df\n",
    "\n",
    "# Apply imputation of the missing values\n",
    "data = impute_missing_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12ca0b-1136-4587-b650-8e33467cd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Encoding categorical variables using pd.get_dummies\n",
    "def encode_categorical_variables_with_dummies(df, target_col='y'):\n",
    "    \"\"\"\n",
    "    Encoding categorical variables using pd.get_dummies (One-Hot Encoding).\n",
    "    :param df: DataFrame.\n",
    "    :param target_col: Name of the target value (Response value).\n",
    "    :return: Encoded DataFrame.\n",
    "    \"\"\"\n",
    "    # Separation of categorical and numerical variables\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and col != target_col]\n",
    "\n",
    "    # Encoding categorical variables\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=False, dtype=int)\n",
    "\n",
    "     # Remove'duration' variable to prevent overfitting\n",
    "    if 'duration' in df_encoded.columns:\n",
    "        df_encoded = df_encoded.drop(columns=['duration'])\n",
    "        print(\"'Duration' variable removed to prevent overfitting.\")\n",
    "\n",
    "    # Conversion of the target value into numeric (0 for 'no', 1 for 'yes')\n",
    "    df_encoded[target_col] = df_encoded[target_col].map({'no': 0, 'yes': 1})\n",
    "\n",
    "    print(\"Categorical variables successfully encoded (One-Hot Encoding).\")\n",
    "    return df_encoded\n",
    "\n",
    "# Apply Encoding for numerical variables\n",
    "data_encoded = encode_categorical_variables_with_dummies(data, target_col = 'y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca3606-5d8c-430d-be11-bac5976824df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Application of SMOTE method to rebalance the classes.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def apply_smote(X, y):\n",
    "    \"\"\"\n",
    "    Apply SMOTE method to rebalance classes.\n",
    "    :param X: Trainning data (features).\n",
    "    :param y: Name of the target value (Response value).\n",
    "    :return: Balanced data.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Visualization of the distrubtion before and after applying SMOTE\n",
    "    print(\"\\nDistribution before SMOTE :\")\n",
    "    print(y.value_counts())\n",
    "\n",
    "    print(\"\\nDistribution after SMOTE :\")\n",
    "    print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=y)\n",
    "    plt.title(\"before SMOTE\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(x=y_resampled)\n",
    "    plt.title(\"after SMOTE\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply class rebalancing with SMOTE\n",
    "X_resampled, y_resampled = apply_smote(data_encoded, data_encoded[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096860f-b13d-4f37-a12c-64abaa8b1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Selection of important features using Random Forest\n",
    "def select_important_features_random_forest(X, y):\n",
    "    \"\"\"\n",
    "    Using Random Forest algorithm to select the most important features. \n",
    "    :param X: Trainning data (features).\n",
    "    :param y: Name of the target value (Response value).\n",
    "    :return: List of selected features \n",
    "    \"\"\"\n",
    "    rf_selector = RandomForestClassifier(random_state=42)\n",
    "    rf_selector.fit(X, y)\n",
    "    feature_importances = rf_selector.feature_importances_\n",
    "    important_features = X.columns[feature_importances > np.mean(feature_importances)].tolist()\n",
    "\n",
    "    # Visualization of feature importance\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    sns.barplot(x=feature_importances, y=X.columns, palette='viridis')\n",
    "    plt.title(\"Importance des Variables selon Random Forest\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Variables\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Features importantes selon Random Forest :\", important_features)\n",
    "    return important_features\n",
    "\n",
    "# Separation of features and target\n",
    "X = data_encoded.drop(columns=['y'])\n",
    "y = data_encoded['y']\n",
    "\n",
    "# Apply selection of important features using Random Forest.\n",
    "important_features_rf = select_important_features_random_forest(X, y)\n",
    "X_rf = X[important_features_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3782a1-e848-493e-9faa-a1b495ab4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Sélection des variables importantes avec ACP\n",
    "def select_important_features_pca(X, explained_variance_threshold=0.95, top_n_per_component=3):\n",
    "    \"\"\"\n",
    "    Use PCA to reduce dimensionality and identify important variables.\n",
    "    :param X: Training data (features).\n",
    "    :param explained_variance_threshold: Explained variance threshold.\n",
    "    :param top_n_per_component: Number of variables to select per principal component.\n",
    "    :return: List of selected variables, DataFrame of variable weights.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=explained_variance_threshold)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    print(f\"Number of retained principal components : {X_pca.shape[1]}\")\n",
    "    print(f\"Cumulative explained variance percentage : {np.sum(pca.explained_variance_ratio_ * 100):.2f}%\")\n",
    "\n",
    "    component_weights = pd.DataFrame(pca.components_, columns=X.columns, index=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "\n",
    "    selected_variables = []\n",
    "    for pc in component_weights.index:\n",
    "        top_variables = component_weights.loc[pc].abs().nlargest(top_n_per_component).index.tolist()\n",
    "        selected_variables.extend(top_variables)\n",
    "        print(f\"\\nTop {top_n_per_component} variables pour {pc}: {top_variables}\")\n",
    "\n",
    "    selected_variables = list(set(selected_variables))\n",
    "    return selected_variables, component_weights\n",
    "\n",
    "# Sélection des variables importantes avec ACP\n",
    "selected_variables_pca, component_weights = select_important_features_pca(X, top_n_per_component=3)\n",
    "print(\"\\nList of variables selected by PCA :\")\n",
    "print(selected_variables_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971871f1-22af-47c4-98ef-014e91498d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Model development and evaluation using ROC AUC and confusion matrix\n",
    "def develop_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics calculation\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.title(f\"Confusion Matrix - {model_name} (ACP)\")\n",
    "        plt.xlabel(\"Predicted values\")\n",
    "        plt.ylabel(\"Actual values\")\n",
    "        plt.show()\n",
    "\n",
    "        # Courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"False Positif Rate\")\n",
    "        plt.ylabel(\"True Positif Rate\")\n",
    "        plt.title(f\"Courbe ROC - {model_name} (ACP)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        results[model_name] = {\"Accuracy\": accuracy, \"ROC-AUC\": roc_auc}\n",
    "\n",
    "    return results\n",
    "\n",
    "# Split Data on tranning and testing sets\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X[selected_variables_pca], y, test_size=0.2, random_state=42)\n",
    "X_train_rf, X_test_rf, _, _ = train_test_split(X[important_features_rf], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model evaluation using ACP\n",
    "print(\"\\nModel evaluation using ACP :\")\n",
    "results_pca = develop_and_evaluate_models(X_train_pca, X_test_pca, y_train, y_test)\n",
    "\n",
    "# Model evaluation using Random Forest\n",
    "print(\"\\nModel evaluation using Random Forest :\")\n",
    "results_rf = develop_and_evaluate_models(X_train_rf, X_test_rf, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aafd3c-ed1e-406a-8fc9-e538303dced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Comparison of results between ACP & Random Forest\n",
    "print(\"\\n--- Comparison of results ---\")\n",
    "\n",
    "print(\"\\Results with ACP :\")\n",
    "for model, metrics in results_pca.items():\n",
    "    print(f\"{model}: Accuracy = {metrics['Accuracy']:.4f}, ROC-AUC = {metrics['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\nResults with Random Forest :\")\n",
    "for model, metrics in results_rf.items():\n",
    "    print(f\"{model}: Accuracy = {metrics['Accuracy']:.4f}, ROC-AUC = {metrics['ROC-AUC']:.4f}\")\n",
    "\n",
    "# Identification of the best model and features selection method\n",
    "best_model_pca = max(results_pca, key=lambda k: results_pca[k]['ROC-AUC'])\n",
    "best_model_rf = max(results_rf, key=lambda k: results_rf[k]['ROC-AUC'])\n",
    "\n",
    "if results_pca[best_model_pca]['ROC-AUC'] > results_rf[best_model_rf]['ROC-AUC']:\n",
    "    print(f\"\\nBest features selection method: ACP with {best_model_pca} (ROC-AUC = {results_pca[best_model_pca]['ROC-AUC']:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nBest features selection method: Random Forest with {best_model_rf} (ROC-AUC = {results_rf[best_model_rf]['ROC-AUC']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
